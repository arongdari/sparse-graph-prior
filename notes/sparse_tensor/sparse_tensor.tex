% !TEX TS-program = pdflatexmk
\documentclass{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsthm}
\usepackage{natbib}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Sparse Graph Prior for Knowledge Graph}
\date{\today}
\author{Dongwoo Kim\\ANU}

\begin{document}

\maketitle

\section{Completely Random Measure}
A completely random measure (CRM) $\mu$ on $\mathbb{R}_+$ is a random measure such that for any countable number of disjoint measurable sets $A_1, A_2, ...$ of $\mathbb{R}_+$, the random variable $\mu(A_1), \mu(A_2), ...$ are independent and $\mu(\cup_i A_i) = \sum_i \mu(A_i)$. If one assumes that the distribution of $\mu([t,s])$ only depends on $t-s$ then the CRM takes the form of $\mu = \sum_{i=1}^{\infty}w_i\delta_{\theta_i}$ where $(w_i, \theta_i)$ are the points of a Poisson point process on $\mathbb{R}_+^2$ with L\'{e}vy intensity measure $\nu(dw, d\theta) = \rho(dw)\lambda(d\theta)$. The Laplace transform of $\mu(A)$ on any measurable set $A$ has a following representation: $\mathbb{E}[e^{-t\mu(A)}] = \exp(-\int_{\mathbb{R}_+ \times A}(1-e^{-tw})\rho(dw)\lambda(d\theta))$ for any $t>0$ and $\rho$ such that $\int_{\mathbb{R}_+}(1-e^{-w})\rho(dw) < \infty$. Laplace exponent is $\psi(t) = \int_{\mathbb{R}} (1 - e^{-t w}) \rho(dw)$.

\section{Caron and Fox Model}

\cite{Caron2015} propose a simple point process on $\mathbb{R}^2$ as a product measure of a complete random measure. They propose a hierarchical model for undirected graphs
\begin{align}
\mu &= \sum_{i=1}^{\infty} w_i \delta_{\theta_i} & &\mu \sim \text{CRM}(\rho, \lambda)\\
D &= \sum_{i,j} n_{ij} \delta_{(\theta_i, \theta_j)} & &D|\mu \sim \text{PP}(\mu \times \mu)\\
Z &=\sum_{i,j} \min(n_{ij} + n_{ji}, 1)\delta_{(\theta_i, \theta_j)}, \label{eqn:cnf}&&
\end{align}
with intensity measure $\nu$ factorising as $\nu(dw, d\theta) = \rho(dw) \lambda(d\theta)$ for a jump part of the measure $\rho$ and Lebesgue measure $\lambda$. $D$ is simply generated from a Poisson process with a product measure as an intensity and can be interpreted as a directed multi-graph.
Given $\mu$, we can directly specify the undirected graph $Z$ as
\[ Pr(z_{ij}=1|w) = 
  \begin{cases}
    1 - \exp(-2w_iw_j)       & \quad i \neq j\\
    1 - \exp(-w_i^2) & \quad i = j.\\
  \end{cases}
\]

They show that the resulting graph is sparse, i.e. \# of edges = $o$(\# of nodes$^2$)\footnote{only counts the nodes which has at least one edge}, if the intensity measure\footnote{This is the L\'{e}vy intensity of the generalised gamma process} is
\begin{align}
\rho(dw) = \frac{1}{\Gamma(1-\sigma)}w^{-1-\sigma}e^{-\tau w}dw,
\end{align}
where the two parameters range
\begin{align}
(\sigma, \tau) \in (0,1) \times [0, +\infty)
\end{align}
and dense if the intensity measure is finite activity, i.e. $\int_{0}^{\infty} \rho(w)dw < \infty$.

The general construction of the sparse graph in Equation \ref{eqn:cnf} results an infinite number of edges due to $\mu(\mathbb{R}_+) = \infty$. A restriction of Lebesgue measure $\lambda$ on $[0, \alpha]$ is used to obtain a finite graph ($\lambda_\alpha = \lambda\delta_{[0, \alpha]}$). Therefore, restricted graph $Z_\alpha$ is defined on the box $[0,\alpha]^2$. We also denote the total mass on $[0, \alpha]^2$ by $Z_\alpha^* = Z_\alpha([0, \alpha]^2)$, and similarly for $D_\alpha^*$ and $\mu_\alpha^*$.

\section{Sparse Prior for Knowledge Graph}
A knowledge base consists of a set of triples (entity, entity, relation) such as (BarackObama, bornIn, Hawaii). The set of triples can be represented as a binary-valued three-way tensor where three dimensions represent entity, entity, and relation, respectively. Here, we directly extend the Caron and Fox's model for the three-way tensor based on two independent completely random measures.
\begin{align}
\mu &= \sum_{i=1}^{\infty} w_i \delta_{\theta_i} & &\mu \sim \text{CRM}(\rho, \lambda)\\
\mu' &= \sum_{k=1}^{\infty} w_k \delta_{\theta_k'} & &\mu' \sim \text{CRM}(\rho', \lambda)\\
D &= \sum_{i,j,k} n_{ijk} \delta_{(\theta_i, \theta_j, \theta_k')}& &D \sim \text{PP}(\mu \times \mu \times \mu') && \\
Z &=\sum_{i,j,k} \min(n_{ijk}, 1)\delta_{(\theta_i, \theta_j, \theta_k')}, &&
\end{align}
where $Z$ is asymmetric in $i$ and $j$ since the knowledge graph is a directed multi-graph. As done in the original model, we can also specify $Z$ as
\[ Pr(z_{ijk}=1|w, w') = 
  \begin{cases}
    1 - \exp(-w_iw_jw_k')       & \quad i \neq j\\
    1 - \exp(-w_i^2w_k') & \quad i = j.\\
  \end{cases}
\]
If we consider $\theta_i$, $\theta_j$, and $\theta_k'$ as nodes in the graph, the above construction will generate a hypergraph where each edge connects three nodes. In the notion of knowledge graphs, it is more intuitive to consider a relation as a type of edge between two entities. In this case, we define two random measures on $\mathbb{R}_+^2$:
\begin{align}
\bar{D} &= \sum_{i,j}\sum_{k}z_{ijk}\delta_{\theta_i,\theta_j}\\
\bar{Z} &= \sum_{i,j}\min(\bar{D}(\{\theta_i, \theta_j\}), 1) \delta_{(\theta_i,\theta_j)},
\end{align}
where $\bar{D}$ is a multigraph, and $\bar{Z}$ is a binary graph of a knowledge base.
\[ Pr(\bar{z}_{ij}=1|w, w') = 
  \begin{cases}
    1 - \exp(-w_iw_j\sum_{k}w_k')       & \quad i \neq j\\
    1 - \exp(-w_i^2\sum_{k}w_k') & \quad i = j.\\
  \end{cases}
\]
To obtain a finite hypergraph (the number of edges is finite), we consider restrictions ${D}_{\alpha\beta}$ and ${Z}_{\alpha\beta}$ to the box $[0,\alpha]^2\times[0,\beta]$. We denote by $Z_{\alpha\beta}^* = Z_{\alpha\beta}([0,\alpha]^2\times[0,\beta])$ the total mass on the restricted area, and similar for $D_{\alpha\beta}^*$ and $\mu_{\alpha}^*$.

\subsection{Generative Process through Urn approach}
Given restriction $\alpha$ and $\beta$, the generative process of $D_{\alpha\beta}$ can be specified as follows:
\begin{enumerate}
\item $\mu_\alpha \sim \text{CRM}(\rho, \lambda_\alpha)$
\item $\mu_\beta' \sim \text{CRM}(\rho', \lambda_\beta)$
\item $D_{\alpha\beta}^* | \mu_\alpha, \mu_\beta' \sim \text{Poisson}(\mu_\alpha^{*2}{\mu'}_\beta^{*})$
\item For $d=1,...,D_{\alpha\beta}^*$:
\begin{enumerate}
\item $\theta_{di} \sim \frac{\mu_\alpha}{\mu_\alpha^*}$
\item $\theta_{dj} \sim \frac{\mu_\alpha}{\mu_\alpha^*}$
\item $\theta_{dk}' \sim \frac{\mu_\beta}{{\mu'}_\beta^{*}}$
\end{enumerate}
\item $D_{\alpha\beta} = \sum_{d=1}^{D_{\alpha\beta}^*} \delta_{(\theta_{di}, \theta_{dj}, \theta_{dk})}$,
\end{enumerate}
where we have used that the total mass of $D_{\alpha\beta}^*$ follows the Poisson distribution. Each node $\theta_i$ is drawn from the normalised CRM (NRM), $\frac{\mu_\alpha}{\mu_\alpha^*}$, which is discrete with probability 1. However, it is not possible to sample $\mu_\alpha$ and $\mu'_\beta$ since these measures have infinite number of atoms. Instead we can simulate finite-dimensional generative process through the urn formulation. Let $\theta_1, ..., \theta_n$ drawn from the normalised CRM $\frac{\mu_\alpha}{\mu_\alpha^*}$. Since NRM is discrete, variables $\theta_1, ..., \theta_n$ takes $l \leq n$ distinct values $\phi_l$, and $m_l$ is the number of variables corresponding to $\phi_l$.
Given total mass $\mu_\alpha^*$ and $\theta_1, ..., \theta_n$, the conditional distribution of $\theta_{n+1}$ can be modelled in terms of exchangeable partition probability function (EPPF):
\begin{align}
\label{eqn:eppf}
\theta_{n+1} | \mu_\alpha^*, \theta_1,...,\theta_n \sim \frac{\Pi_{n+1}^{l+1}(m_1, ..., m_l, 1 | \mu_\alpha^*)}{\Pi_{n}^{l}(m_1, ..., m_l | \mu_\alpha^*)} \frac{1}{\alpha} \lambda_\alpha
+ \sum_{i=1}^{l}\frac{\Pi_{n+1}^{l}(m_1, ..., m_{i}+1, ..., m_l | \mu_\alpha^*)}{\Pi_{n}^{l}(m_1, ..., m_l| \mu_\alpha^*)} \delta_{\phi_l}
\end{align}
where
\begin{align}
\Pi_{n}^l(m_1, ..., m_l|\mu_\alpha^*) = \frac{\sigma^l \mu_\alpha^{*-n}}{\Gamma(n-l\sigma)g_{\sigma}(\mu_\alpha^*)} \int_{0}^{\mu_\alpha^*}s^{n-l\sigma-1}g_{\sigma}(\mu_\alpha^*-s)ds \bigg(\prod_{i=1}^{l} \frac{\Gamma(m_i-\sigma)}{\Gamma(1-\sigma)} \bigg),
\end{align}
and $g_\sigma$ is the pdf of the positive stable distribution.
Finally, the total mass of $\mu_\alpha^*$ and ${\mu'}_\beta^{*}$ follows an exponentially tilted stable distribution where the exact sampler exists \citep{devroye2009random,hofert2011sampling}. 

Using this urn representation, we can rewrite the generative process as
\begin{enumerate}
\item $\mu_\alpha^* \sim P_{\mu_\alpha^*}$
\item ${\mu'}_\beta^{*} \sim P_{{\mu'}_\beta^{*}}$
\item $D_{\alpha\beta}^* | \mu_\alpha, \mu_\beta' \sim \text{Poisson}(\mu_\alpha^{*2}{\mu'}_\beta^{*})$
\item For $d=1,...,D_{\alpha\beta}^*$:
\begin{enumerate}
\item Sample $\theta_{di}$, $\theta_{dj}$, and $\theta_{dk}'$ with Urn process in Eqn \ref{eqn:eppf}
\end{enumerate}
\item $D_{\alpha\beta} = \sum_{d=1}^{D_{\alpha\beta}^*} \delta_{(\theta_{di}, \theta_{dj}, \theta_{dk})}$,
\end{enumerate}

\subsection{Sparsity}

\begin{theorem} \label{thm:edge} Consider the point process $\bar{Z}$ with infinite-activity intensity measures $\rho(dw)$ and $\rho'(dw')$. Given $\mu'$ from $\rho'(dw')$, the number of edges in $\bar{Z}_{\alpha}$ grows quadratically as $\alpha \rightarrow \infty$ almost surely.
\end{theorem}
\begin{proof}
$\sum_{k=1}^{\infty} w_k' < \infty$ a.s. When $\mu'$ is given and the sum of $w_k'$ is finite a.s., we can use the same proof technique used in \cite{Caron2015}.
\end{proof}
What if $\mu'$ is not given? Let $(X_i)$ and $(Y_k)$ be i.i.d. real-valued random variable from $p$ and $q$, respectively, and let $h(x_1, x_2, y_1)$ be a measurable function symmetric in the first two arugments. 
\begin{align}
\frac{2 \sum_{i<j}\sum_{k} h(X_i, X_j, Y_k)}{n_x(n_x -1) n_y} \xrightarrow[]{?} \mathbb{E}[h(X_i, X_j, Y_k)]\quad a.s.\quad as \quad n\rightarrow \infty
\end{align}
If this strong law of the large numbers for two samples is correct, we may proof Theorem 3.1 in more general case ($\mu'$ is not given).

\begin{theorem} Consider the point process $\bar{Z}$ with infinite-activity intensity measures $\rho(dw)$ and $\rho'(dw')$. Let $N_\alpha$ be a number of nodes having at least one connection. Given $\mu'$ from $\rho'(dw')$, the number of nodes $N_\alpha$ in $\bar{Z}_{\alpha}$ grows superlinearly as $\alpha \rightarrow \infty$ almost surely.
\end{theorem}
\begin{proof}
As \ref{thm:edge}.
\end{proof}

\subsection{Posterior inference}
We first characterise the posterior of $\mu_\alpha$ given $\mu'_\beta$ and $D_{\alpha\beta}$. The conditional Laplace functional of $\mu_\alpha$ given $D_{\alpha\beta}$ is $\mathbb{E}[e^{-\mu_\alpha(f)}|\mu'_\beta, D_{\alpha\beta}]$, for any non-negative measurable function $f$ such that $\mu_\alpha(f) = \sum_{i=1}^{\infty}w_i f(\theta_i)$. We have $\mu_\alpha(f) = \Pi(\tilde{f})$ where $\Pi = \sum_{i=1}^{\infty} \delta_{w_i, \theta_i}$ is a Poisson random measure on $\mathcal{S} = (0, \infty) \times [0, \alpha]$ with mean measure $\rho \times \lambda$ and $\tilde{f}(w, \theta) = wf(\theta)$. Let $n_{i**} = \sum_{j=1}^{N_\alpha}\sum_{k=1}^{N_\beta}n_{ijk}$, $m_i = \sum_{j=1}^{N_\alpha}\sum_{k=1}^{N_\beta} n_{ijk} + n_{jik}$, and $m'_k = \sum_{i=1}^{N_\alpha} \sum_{j=1}^{N_\alpha} n_{ijk}$.

\begin{align}
\label{eqn:lpl_d}
\mathbb{E}_{\mu_\alpha}[e^{-\mu_\alpha(f)}|D_{\alpha\beta}, \mu'_\beta] 
&= \mathbb{E}_{\Pi}[e^{-\int \tilde{f}(w, \theta)\Pi(dw, d\theta)}|D_{\alpha\beta}, \mu'_\beta] \\
&= \frac{\mathbb{E}_{\Pi}[ e^{-\Pi(\tilde{f})} P(D_{\alpha\beta}|\Pi, \mu'_\beta)]}{\mathbb{E}_{\Pi}[P(D_{\alpha\beta}|\Pi, \mu'_\beta)]}\\
%&= \frac{\mathbb{E}_{\Pi}[e^{-\Pi(\tilde{f})} e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} w_i^{m_i} \prod_{k=1}^{N_\beta} {w'_k}^{m_k} ]}{\mathbb{E}_{\Pi}[e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} w_i^{m_i} \prod_{k=1}^{N_\beta} {w'_k}^{m_k} ]]}
&= \frac{\mathbb{E}_{\Pi}[e^{-\Pi(\tilde{f})} e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} w_i^{m_i}]}{\mathbb{E}_{\Pi}[e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} w_i^{m_i}]]}
\end{align}
where $h(w, \theta) = w$ and
\begin{align}
P(D_{\alpha\beta}|\Pi, \mu'_\beta) & = P(D_{\alpha\beta}|\mu_\alpha, \mu'_\beta)\\
& = \text{Poisson}(D^*_{\alpha\beta}|\mu_\alpha^{*2}{\mu'}_\beta^{*}) 
\prod_{i=1}^{N_\alpha} P(n_{i**}|\mu_\alpha) \prod_{j=1}^{N_\alpha} P(n_{*j*}|\mu_\alpha)
\prod_{k=1}^{N_\beta} P(n_{**k}|\mu_\beta) \\
& = \frac{ (\mu_\alpha^{*2}{\mu'}_\beta^{*})^{D^*_{\alpha\beta}} e^{-\mu_\alpha^{*2}{\mu'}_\beta^{*}} }{D^*_{\alpha\beta}!}
\prod_{i=1}^{N_\alpha} \Big( \frac{w_i}{\mu_\alpha^*} \Big)^{n_{i**}} 
\prod_{j=1}^{N_\alpha} \Big( \frac{w_j}{\mu_\alpha^*} \Big)^{n_{*j*}} 
\prod_{k=1}^{N_\beta} \Big( \frac{w'_k}{{\mu'}_\beta^{*}} \Big)^{n_{**k}}\\
& =\frac{e^{-\mu_\alpha^{*2}{\mu'}_\beta^{*}} }{D^*_{\alpha\beta}!}
\prod_{i=1}^{N_\alpha} w_i^{m_i}
\prod_{k=1}^{N_\beta} {w'_k}^{m_k} 
 =\frac{e^{-\Pi(h)^2{\mu'}_\beta^{*}} }{D^*_{\alpha\beta}!}
\prod_{i=1}^{N_\alpha} w_i^{m_i}
\prod_{k=1}^{N_\beta} {w'_k}^{m_k}\\
\\
\mu_\alpha^* & = \sum_{i=1}^{\infty} w_i, \qquad {\mu'}_\beta^{*} = \sum_{k=1}^{\infty} w'_k = \sum_{k=1}^{N_\beta} w'_k + {w'}^*
\end{align}
Applying the generalised Palm formula to the numerator yields
\begin{align}
&\mathbb{E}_{\Pi}\Big[e^{-\Pi(\tilde{f})} e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} w_i^{m_i} \Big] &\\
&= \mathbb{E}_{\Pi}\Big[e^{-\Pi(\tilde{f})} e^{-\Pi(h)^2{\mu'}_\beta^{*}} \prod_{i=1}^{N_\alpha} \sum_{w_j, \vartheta_j \in \Pi} w_j^{m_i} \mathbf{1}_{\theta_i}(\vartheta_j)\Big]& \\
&= \mathbb{E}_{\Pi}\Big[\int_{\mathcal{S}^{N_\alpha}} e^{-\Pi(\tilde{f})} e^{-\Pi(h)^2{\mu'}_\beta^{*}}  \prod_{i=1}^{N_\alpha} w_j^{m_i} \mathbf{1}_{\theta_i}(\vartheta_j) \Pi(dw_j, d\vartheta_j)\Big]& \\
&=  \int_{\mathcal{S}^{N_\alpha}} \mathbb{E}_{\Pi}\Big[ e^{-(\Pi+\sum_i^{N_\alpha}\delta_{(w_i, \theta_i)})(\tilde{f})} e^{-(\Pi+\sum_i^{N_\alpha}\delta_{(w_i, \theta_i)})(h)^2{\mu'}_\beta^{*}}\Big] \prod_{i=1}^{N_\alpha} w_j^{m_i} \mathbf{1}_{\theta_i}(\vartheta_j) \rho(dw_j)\lambda(d\vartheta_j) & \\
&=  \int_{\mathcal{S}^{N_\alpha}} \mathbb{E}_{\mu_\alpha}\Big[ e^{-\mu_\alpha(f) - \sum_{i=1}^{N_\alpha}w_if(\vartheta_j)} e^{-(\mu_\alpha(1) + \sum_{i=1}^{N_\alpha} w_i)^2{\mu'}_\beta^{*}}\Big] \prod_{i=1}^{N_\alpha} w_j^{m_i} \mathbf{1}_{\theta_i}(\vartheta_j) \rho(dw_j)\lambda(d\vartheta_j) &\\
&=  \int_{\mathcal{S}^{N_\alpha}} \mathbb{E}_{\mu_\alpha^*}\bigg[ \mathbb{E}_{\mu_\alpha} \Big[e^{-\mu_\alpha(f)}|\mu_\alpha^* \Big] e^{- \sum_{i=1}^{N_\alpha}w_if(\vartheta_j)} e^{-(\mu_\alpha^* + \sum_{i=1}^{N_\alpha} w_i)^2{\mu'}_\beta^{*}}\bigg] \prod_{i=1}^{N_\alpha} w_j^{m_i} \mathbf{1}_{\theta_i}(\vartheta_j) \rho(dw_j)\lambda(d\vartheta_j) &
\end{align}
The denominator is obtained by taking $f=0$.
\begin{align}
\mathbb{E}_{\mu_\alpha}[e^{-\mu_\alpha(f)}|D_{\alpha\beta}, \mu'_\beta] &= \int_{\mathbb{R}^{N_\alpha + 1}} E_{\mu_\alpha}[e^{-\mu_\alpha(f)}|\mu_\alpha^* = w^*]\\
&\quad \times e^{\sum_{i=1}^{N_\alpha}w_i f(\theta_i)} p(w_1, ..., w_{N_\alpha}, w^*|D_{\alpha\beta}, \mu_\beta) dw_{1:N_\alpha}dw^*
\end{align}
where
\begin{align}
p(w_1, ..., w_{N_\alpha}, w^*|D_{\alpha\beta}, \mu_\beta)&
= \frac{\prod_{i=1}^{N_\alpha} w_j^{m_i} \rho(w_i) e^{-(w^* + \sum_{i=1}^{N_\alpha} w_i)^2{\mu'}_\beta^{*}} g^*_\alpha(w^*)}
{\int_{\mathbb{R}^{N_\alpha + 1}}\prod_{i=1}^{N_\alpha} \tilde{w}_j^{m_i} \rho(\tilde{w}_i) e^{-(\tilde{w}^* + \sum_{i=1}^{N_\alpha} \tilde{w}_i)^2{\mu'}_\beta^{*}} g^*_\alpha(\tilde{w}^*) d\tilde{w}_{1:N_\alpha}d\tilde{w}^*}\\
\end{align}
$g^*_\alpha(w^*)$ is a density function of random variable $w^*$ of which Laplace transform is $\mathbb{E}[e^{tw^*}] = e^{\alpha\psi(t)}$. Therefore, the conditional of $\mu_\alpha$ given $D_{\alpha\beta}, \mu'_\beta$ is
\begin{align}
w^* \sum_{i=1}^{\infty}\tilde{P}_i\delta_{\tilde\theta_i} + \sum_{i=1}^{N_\alpha} w_i \delta_{\theta_i} 
\end{align}
where $(\tilde{P})$ are distributed from a Poisson-Kingman distribution conditional on $w^*$, and the weights $w_1, ..., w_{N_\alpha}, w^*$ are jointly dependent conditional on $D_{\alpha\beta}$ and $\mu'_\beta$:
\begin{align}
p(w_1, ..., w_{N_\alpha}, w^* | D_{\alpha\beta}, \mu'_\beta) \propto \prod_{i=1}^{N_\alpha}{w_i}^{m_i} e^{(-w_* + \sum_{i=1}^{N_\alpha}w_i)^2 {\mu'}_\beta^*} \prod_{i=1}^{N_\alpha} \rho(w_i) g^*_\alpha(w^*)
\end{align}

The conditional Laplace functional of $\mu'_\beta$ given $\mu_\alpha$ and $D_{\alpha\beta}$ can be carried out in the same way as we've done in $\mu_\alpha$.

\bibliographystyle{apalike}
\bibliography{ref}

\end{document}
